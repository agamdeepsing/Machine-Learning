
# Ques 1
The likelihood of an outcome in a given dataset is represented by prior probability. In the mortgage situation, for example, P(Y) is the 2% default rate on a house mortgage. P(Y|X) is known as the conditional probability because it expresses the likelihood of an outcome given the evidence, that is, when the value of X is known.
    


# Ques 2
A posterior probability is a revised probability that takes new available information into account. Assume there are two urns, urn A with 5 black balls and 10 red balls and urn B with 10 black balls and 5 red balls.


# Ques 3
In Machine Learning and Data Science, the probabilistic Function represents the dataset's joint distribution of probabilities (jpd) as a function of the parameter. Consider it the likelihood of receiving the observed data given the parameter values.


# Ques 4
Naive Bayes is a straightforward and effective predictive modelling technique. The term "naive Bayes" refers to the assumption that each input variable is independent. This is a strong assumption that is unreasonable for real-world data; yet, the approach is extremely successful on a wide range of complicated situations.

# ques 5
The Bayes Optimal Classifier is a probabilistic model that predicts the most likely outcome for a new case. The Bayes Optimal Classifier is a probabilistic model that uses the training data and space of hypotheses to create a prediction for a new data instance.


# ques 6
A probability distribution for each conceivable hypothesis based on observable data. New cases can be categorised by combining the probability of various hypothesis' predictions.

# Ques 7
Consistent Learners: A learner L with a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis.

# Ques 8
This method is fast and can save you a lot of time. Naive Bayes may be used to solve multi-class prediction issues. If the premise of feature independence remains true, it can outperform other models while using far less training data.


# Ques 9
The naive Bayes classifier's major drawback is that it is based on the frequently incorrect assumption of equally relevant and independent features, resulting in biassed posterior probability.

# Ques 10
1.text classifiaction
The Naive Bayes classifier is a basic classifier that classifies events based on their probability. It is often used for text categorization. We can train a Naive Bayes classifier using the training set to automatically categorise a new phrase.

2.spam filtering
Naive Bayes classifiers function by connecting the usage of tokens (usually words, but sometimes other items) with spam and non-spam e-mails, and then applying Bayes' theorem to compute the likelihood that an email is or is not spam. It is one of the earliest methods of spam filtering, dating back to the 1990s.

3.market sentiment analysis
Market sentiment analysis is the extraction of subjective emotions and sensations from text. One popular application of sentiment analysis is to determine if a text exhibits negative or positive emotions. Naive Bayes is a prominent text classification technique.